/****TIPS*****/
- make reward function as smooth as possible
- make the gradient increases when getting closer to the goal
- positive rewards: keep going to to accumulate reward
- negative rewards: reach a terminal state as quickly as possible to avoid accumulating penalties. 
- If you are going to use negative terminals, you should stick to positive rewards, or at least
  a reward that will end up positive on most episodes given just random exploration. 
- For numerical reasons, it is best to bound the reward between [-1,1].
- If you want the agent to learn some kinda information, you need to give it to him through either
  reward or even a state. 
- You can make your reward a step function if the agent needs to achieve multiple tasks before
  reaching its goal. if you reward is complex, BREAK IT DOWN INTO MULTIPLE CONCEPTS AND REWARD 
  THE AGENT FOR EACH CONCEPT.
- mutiplying/shifting the WHOLE reward by a constant does not the behavior you're trying to impose.

/****DIFFERENT APPROACHES*******/
- The main part about PBRS is adding a constrained reward-shaping function, i.e. it leaves the 
  optimal policy invarient. 
