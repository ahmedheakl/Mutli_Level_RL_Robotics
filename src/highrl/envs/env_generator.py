"""Implementation for the environment to train the environmenet generator modle

The model is assumed to take the difficulty (generated by the teacher policy) and
generate a robot-environment abiding by the generated difficulty. 

In this module, we implement the environment generator using reinforcement learninig, 
for not constraining the creativity the model, i.e. there are numerous ways to generate
environments that abide by a certain difficulty.
"""
from typing import Tuple, Callable, List
from gym import Env, spaces
import numpy as np
import torch
from torch import nn
from stable_baselines3 import PPO  # type: ignore
from stable_baselines3.common.policies import ActorCriticPolicy


class EnvGeneratorPolicy(nn.Module):
    """Implementation for the environment generator model"""

    hidden_size = 16
    hard_obs = 2
    med_obs = 3
    small_obs = 4
    device = "cuda"

    def __init__(
        self,
        feature_dim: int = 1,
        last_layer_dim_pi: int = 40,
        last_layer_dim_vf: int = 32,
    ) -> None:
        super().__init__()
        # IMPORTANT:
        # Save output dimensions, used to create the distributions
        self.latent_dim_pi = last_layer_dim_pi
        self.latent_dim_vf = last_layer_dim_vf

        self.value_net = nn.Sequential(
            nn.Linear(feature_dim, last_layer_dim_vf),
            nn.ReLU(),
        )

        self.policy_net = nn.Sequential(
            nn.Linear(feature_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, last_layer_dim_pi),
            nn.Sigmoid(),
        )
        # # This linear layer will be used to map from the hidden dimension
        # # to the output dimension.
        # self.out = nn.Linear(self.hidden_size, last_layer_dim_pi)

        # self.lstm = nn.LSTM(last_layer_dim_pi, self.hidden_size)

    def forward(self, features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """Forward pass through the value network and policy network"""
        return self.forward_actor(features), self.forward_critic(features)

    def _init_hidden(self, batch_size: int = 1) -> Tuple[torch.Tensor, torch.Tensor]:
        """Initialize the hidden and cell gate tensors for the LSTM layer

        Note here we are assuming that the `num_layers` and `batch_size`
        are equal to one.
        """
        return (
            torch.zeros(1, batch_size, self.hidden_size, device=self.device),
            torch.zeros(1, batch_size, self.hidden_size, device=self.device),
        )

    def forward_actor(self, features: torch.Tensor) -> torch.Tensor:
        """Forward pass through the actor network"""
        return self.policy_net(features)

    # def forward_actor(self, features: torch.Tensor) -> torch.Tensor:
    #     """Forward pass through the actor network"""

    #     # Features dim = [batch_size, features_dim]
    #     # NOTE: WE ASSUME THAT THE BATCH_SIZE=1

    #     # Here we are assuming that the model will build any environment
    #     # with 4 basic components: robot/goal, hard obstacles, medium obstacles,
    #     # and small obstacles.
    #     num_outputs = 1 + self.hard_obs + self.med_obs + self.small_obs
    #     outputs = []
    #     batch_size = features.size(0)
    #     hidden, cell = self._init_hidden(batch_size)

    #     # Repeating the difficulty value to match the output dimension
    #     # Output Dim = [1, batch_size, output_size]
    #     output = features.repeat(batch_size, self.latent_dim_pi).unsqueeze(0)
    #     for _ in range(num_outputs):
    #         print("output:", output.shape, "hidden:", hidden.shape, "cell:", cell.shape)
    #         output, (hidden, cell) = self.lstm(output, (hidden, cell))
    #         output = self.out(output.view(batch))
    #         # Output dim = [1, 4]
    #         outputs.append(output.)

    #     # Action dim = [batch_size, 4 * (num_outputs+1)]
    #     out_features = torch.concat(outputs, dim=1).view(batch_size, -1)
    #     print(out_features.shape)
    #     return out_features

    def forward_critic(self, features: torch.Tensor) -> torch.Tensor:
        """Forward pass through the value network"""
        return self.value_net(features)


class EnvGeneratorActorCritic(ActorCriticPolicy):
    """Actor critic architecture implementation for the environment
    generation model"""

    def __init__(
        self,
        observation_space: spaces.Space,
        action_space: spaces.Space,
        lr_schedule: Callable[[float], float],
        *args,
        **kwargs,
    ):

        super().__init__(
            observation_space,
            action_space,
            lr_schedule,
            # Pass remaining arguments to base class
            *args,
            **kwargs,
        )
        # Disable orthogonal initialization
        self.ortho_init = False

    def _build_mlp_extractor(self) -> None:
        self.mlp_extractor = EnvGeneratorPolicy(self.features_dim)


class GeneratorEnv(Env):
    """Gym environment implementation to train the environment generator model"""

    max_difficulty = 256 * 256  # width * height

    def __init__(self, max_obstacles: int = 14):
        super().__init__()
        self.action_space = spaces.Box(
            low=0.0,
            high=1.0,
            shape=(4 * max_obstacles,),
            dtype=np.float32,
        )
        self.observation_space = spaces.Box(
            low=0,
            high=self.max_difficulty,
            shape=(1,),
            dtype=np.float32,
        )

    def step(self, action: torch.Tensor) -> Tuple[List[int], np.float32, bool, dict]:
        """Step through the environment and return the next observation"""
        print("ACTION", action.shape)
        return self._make_obs(), np.float32(0.0), False, {}

    def _make_obs(self) -> List[int]:
        """Create observations for the environment"""
        return [1]

    def reset(self):
        """Reset environemnt and return start observation"""
        return self._make_obs()

    def render(self):
        """Overriding render method"""
        return super().render()


def main() -> None:
    """Main method for starting the training for the envirioment
    generator model"""
    hard_obs = EnvGeneratorPolicy.hard_obs
    med_obs = EnvGeneratorPolicy.med_obs
    small_obs = EnvGeneratorPolicy.small_obs
    env = GeneratorEnv(hard_obs + med_obs + small_obs + 1)
    model = PPO(EnvGeneratorActorCritic, env, verbose=1, device="cuda")
    model.learn(5000)


if __name__ == "__main__":
    main()
