"""Implementation for the environment to train the environmenet generator modle

The model is assumed to take the difficulty (generated by the teacher policy) and
generate a robot-environment abiding by the generated difficulty.

In this module, we implement the environment generator using reinforcement learninig,
for not constraining the creativity the model, i.e. there are numerous ways to generate
environments that abide by a certain difficulty.
"""
from typing import Tuple, Callable, List
import logging
import random
from gym import Env, spaces
import numpy as np
import torch
from torch.utils.tensorboard import SummaryWriter  # type: ignore
from torch import nn
from stable_baselines3 import PPO  # type: ignore
from stable_baselines3.common.policies import ActorCriticPolicy

from highrl.obstacle.obstacles import Obstacles
from highrl.obstacle.single_obstacle import SingleObstacle
from highrl.agents.robot import Robot
from highrl.utils import Position
from highrl.utils.teacher_checker import compute_difficulty
from highrl.utils.logger import init_logger

_LOG = logging.getLogger(__name__)


class EnvGeneratorPolicy(nn.Module):
    """Implementation for the environment generator model"""

    hidden_size = 16
    hard_obs = 2
    med_obs = 3
    small_obs = 4
    device = "cuda"
    output_size = 4

    def __init__(
        self,
        feature_dim: int = 1,
        last_layer_dim_pi: int = 40,
        last_layer_dim_vf: int = 32,
    ) -> None:
        super().__init__()
        # IMPORTANT:
        # Save output dimensions, used to create the distributions
        self.latent_dim_pi = last_layer_dim_pi
        self.latent_dim_vf = last_layer_dim_vf

        self.value_net = nn.Sequential(
            nn.Linear(feature_dim, last_layer_dim_vf),
            nn.ReLU(),
        )

        # This linear layer will be used to map from the hidden dimension
        # to the output dimension.
        self.out = nn.Linear(self.hidden_size, self.output_size)

        self.lstm = nn.LSTM(self.output_size, self.hidden_size)

    def forward(self, features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """Forward pass through the value network and policy network"""
        return self.forward_actor(features), self.forward_critic(features)

    def _init_hidden(self, batch_size: int = 1) -> Tuple[torch.Tensor, torch.Tensor]:
        """Initialize the hidden and cell gate tensors for the LSTM layer

        Note here we are assuming that the `num_layers` and `batch_size`
        are equal to one.
        """
        return (
            torch.zeros(1, batch_size, self.hidden_size, device=self.device),
            torch.zeros(1, batch_size, self.hidden_size, device=self.device),
        )

    def forward_actor(self, features: torch.Tensor) -> torch.Tensor:
        """Forward pass through the actor network"""

        # Features dim = [batch_size, features_dim]
        # NOTE: WE ASSUME THAT THE BATCH_SIZE=1

        # Here we are assuming that the model will build any environment
        # with 4 basic components: robot/goal, hard obstacles, medium obstacles,
        # and small obstacles.
        num_outputs = 1 + self.hard_obs + self.med_obs + self.small_obs
        outputs = []
        batch_size = features.size(0)
        hidden, cell = self._init_hidden(batch_size)
        sigmoid = nn.Sigmoid()

        # Repeating the difficulty value to match the output dimension
        # Output Dim = [batch_size, output_size]
        output = features.repeat(batch_size, self.output_size)

        # Filling initial input as follows [difficulty, -1, -1, -1]
        # so that the model would know that this the initial value
        for batch, _ in enumerate(output):
            for value_idx in range(1, 4):
                output[batch][value_idx] = -1

        for _ in range(num_outputs):
            # Output Dim = [1, batch_size, hidden_size]
            output, (hidden, cell) = self.lstm(output.unsqueeze(0), (hidden, cell))

            # Dim = [batch_size, output_size]
            output = self.out(output.squeeze(0))

            # Running through sigmoid activation for output in range [0, 1]
            outputs.append(sigmoid(output))

        # Action dim = [batch_size, 4 * (num_outputs+1)]
        out_features = torch.concat(outputs, dim=1)
        return out_features

    def forward_critic(self, features: torch.Tensor) -> torch.Tensor:
        """Forward pass through the value network"""
        return self.value_net(features)


class EnvGeneratorActorCritic(ActorCriticPolicy):
    """Actor critic architecture implementation for the environment
    generation model"""

    hard_size = 50
    med_size = 40
    small_size = 30

    def __init__(
        self,
        observation_space: spaces.Space,
        action_space: spaces.Space,
        lr_schedule: Callable[[float], float],
        *args,
        **kwargs,
    ):

        super().__init__(
            observation_space,
            action_space,
            lr_schedule,
            *args,
            **kwargs,
        )
        # Disable orthogonal initialization
        self.ortho_init = False

    def _build_mlp_extractor(self) -> None:
        # pylint: disable=attribute-defined-outside-init
        self.mlp_extractor = EnvGeneratorPolicy()


class GeneratorEnv(Env):
    """Gym environment implementation to train the environment generator model"""

    width = 256
    height = 256
    max_difficulty = width * height  # width * height

    def __init__(self, max_obstacles: int = 10):
        super().__init__()
        self.action_space = spaces.Box(
            low=0.0,
            high=1.0,
            shape=(4 * max_obstacles,),
            dtype=np.float32,
        )
        self.observation_space = spaces.Box(
            low=0,
            high=self.max_difficulty,
            shape=(1,),
            dtype=np.float32,
        )
        self.difficulty: float = 0
        self.time_step: int = 0
        self.tb_writer = SummaryWriter("runs")

    def step(self, action: np.ndarray) -> Tuple[List[float], float, bool, dict]:
        """Step through the environment and return the next observation

        Action shape = (40, ). All values are between [0, 1]
        """
        robot_pos = action[:4]
        rob_x = robot_pos[0] * self.width
        rob_y = robot_pos[1] * self.height
        goal_x = robot_pos[2] * self.width
        goal_y = robot_pos[3] * self.height
        robot = Robot(Position[float](rob_x, rob_y), Position[float](goal_x, goal_y))
        obstacles_ls = []

        # Convert obstacles position/dimension from [0, 1] to [0, width]
        for idx in range(4, 40, 4):
            dims = [action[idx + dim_i] * self.width for dim_i in range(4)]
            obstacles_ls.append(SingleObstacle(*dims))
        obstacles = Obstacles(obstacles_ls)

        # Compute the difficulty of the generated environment
        old_difficulty = self.difficulty
        self.difficulty, _ = compute_difficulty(
            obstacles,
            robot,
            self.width,
            self.height,
        )
        reward: float = -abs(self.difficulty - old_difficulty)
        self.tb_writer.add_scalar("generator_reward", reward, self.time_step)
        self.time_step += 1
        _LOG.info("Reward %f", reward)

        # Notice here we are returning done=True, so that the model would update
        # its weights after each step, as it is considering each step as separate
        # episode
        return self._make_obs(), reward, True, {}

    def _make_obs(self) -> List[float]:
        """Create observations for the environment"""

        # Generating a new random difficulty value as an observation for the model
        self.difficulty = random.random() * self.max_difficulty
        return [self.difficulty]

    def reset(self):
        """Reset environemnt and return start observation"""
        return self._make_obs()

    # pylint: disable=arguments-differ
    def render(self):
        """Overriding render method"""
        return super().render()


def main() -> None:
    """Main method for starting the training for the envirioment
    generator model"""
    init_logger()
    hard_obs = EnvGeneratorPolicy.hard_obs
    med_obs = EnvGeneratorPolicy.med_obs
    small_obs = EnvGeneratorPolicy.small_obs
    env = GeneratorEnv(hard_obs + med_obs + small_obs + 1)
    model = PPO(EnvGeneratorActorCritic, env, verbose=1, device="cuda")
    model.learn(5000)
    model.save("generator_model/")


if __name__ == "__main__":
    main()
